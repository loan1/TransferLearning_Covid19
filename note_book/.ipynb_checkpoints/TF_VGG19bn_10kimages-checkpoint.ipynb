{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset: https://drive.google.com/file/d/18aMf57_1u2AWInnMB67s3Xku0sPzm28u/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#for Data preprocessing and Augmentation\n",
    "import os\n",
    "from imutils import paths\n",
    "import cv2\n",
    "# import Augmentor\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#for reading and displaying images\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "#Pytorch libraries and modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch.nn import Linear, CrossEntropyLoss\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#torchvision for pre-trained models\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "\n",
    "#for evaluating model\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "# from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "classes = ['Normal', 'Covid']\n",
    "num_classes = 2\n",
    "num_epochs = 15\n",
    "criterion = CrossEntropyLoss()\n",
    "CHECKPOINT_PATH = './FT_VGG19bn1.pt'\n",
    "path_dataset = '../dataset10K_images/'\n",
    "\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal = path_dataset + 'NORMAL/'\n",
    "covid = path_dataset + 'COVID/'\n",
    "\n",
    "dir_normal = os.listdir(normal)\n",
    "dir_covid = os.listdir(covid)\n",
    "\n",
    "# dataset = np.concatenate(dir_normal, dir_covid, axis = 0)\n",
    "# dataset\n",
    "dict_data0 = {'path': normal, 'image_name': dir_normal, 'labels': 0}\n",
    "dict_data1 = {'path': covid, 'image_name': dir_covid, 'labels': 1}\n",
    "\n",
    "# len(dict_data['COVID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df0 = pd.DataFrame(dict_data0)\n",
    "df1 = pd.DataFrame(dict_data1)\n",
    "df = pd.concat([df0, df1])\n",
    "df.to_csv('./data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data=train_test_split(df, test_size=0.2, random_state = 42, shuffle=True)\n",
    "train_data, val_data =train_test_split(train_data, test_size=0.2, random_state = 42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.1254902 , 0.1254902 , 0.1254902 ],\n",
       "         [0.1254902 , 0.1254902 , 0.1254902 ],\n",
       "         [0.1254902 , 0.1254902 , 0.1254902 ],\n",
       "         ...,\n",
       "         [0.1254902 , 0.1254902 , 0.1254902 ],\n",
       "         [0.11764706, 0.11764706, 0.11764706],\n",
       "         [0.1372549 , 0.1372549 , 0.1372549 ]],\n",
       "\n",
       "        [[0.1254902 , 0.1254902 , 0.1254902 ],\n",
       "         [0.1254902 , 0.1254902 , 0.1254902 ],\n",
       "         [0.11372549, 0.11372549, 0.11372549],\n",
       "         ...,\n",
       "         [0.11764706, 0.11764706, 0.11764706],\n",
       "         [0.12941176, 0.12941176, 0.12941176],\n",
       "         [0.12941176, 0.12941176, 0.12941176]],\n",
       "\n",
       "        [[0.1254902 , 0.1254902 , 0.1254902 ],\n",
       "         [0.1254902 , 0.1254902 , 0.1254902 ],\n",
       "         [0.13333333, 0.13333333, 0.13333333],\n",
       "         ...,\n",
       "         [0.1254902 , 0.1254902 , 0.1254902 ],\n",
       "         [0.13333333, 0.13333333, 0.13333333],\n",
       "         [0.12156863, 0.12156863, 0.12156863]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.1254902 , 0.1254902 , 0.1254902 ],\n",
       "         [0.11764706, 0.11764706, 0.11764706],\n",
       "         [0.10980392, 0.10980392, 0.10980392],\n",
       "         ...,\n",
       "         [0.12941176, 0.12941176, 0.12941176],\n",
       "         [0.12941176, 0.12941176, 0.12941176],\n",
       "         [0.1254902 , 0.1254902 , 0.1254902 ]],\n",
       "\n",
       "        [[0.11372549, 0.11372549, 0.11372549],\n",
       "         [0.10588235, 0.10588235, 0.10588235],\n",
       "         [0.11372549, 0.11372549, 0.11372549],\n",
       "         ...,\n",
       "         [0.1254902 , 0.1254902 , 0.1254902 ],\n",
       "         [0.12941176, 0.12941176, 0.12941176],\n",
       "         [0.1372549 , 0.1372549 , 0.1372549 ]],\n",
       "\n",
       "        [[0.12156863, 0.12156863, 0.12156863],\n",
       "         [0.12156863, 0.12156863, 0.12156863],\n",
       "         [0.11372549, 0.11372549, 0.11372549],\n",
       "         ...,\n",
       "         [0.12941176, 0.12941176, 0.12941176],\n",
       "         [0.12156863, 0.12156863, 0.12156863],\n",
       "         [0.13333333, 0.13333333, 0.13333333]]],\n",
       "\n",
       "\n",
       "       [[[0.15686275, 0.15686275, 0.15686275],\n",
       "         [0.14117647, 0.14117647, 0.14117647],\n",
       "         [0.11764706, 0.11764706, 0.11764706],\n",
       "         ...,\n",
       "         [0.43529412, 0.43529412, 0.43529412],\n",
       "         [0.4627451 , 0.4627451 , 0.4627451 ],\n",
       "         [0.40784314, 0.40784314, 0.40784314]],\n",
       "\n",
       "        [[0.15294118, 0.15294118, 0.15294118],\n",
       "         [0.12941176, 0.12941176, 0.12941176],\n",
       "         [0.12156863, 0.12156863, 0.12156863],\n",
       "         ...,\n",
       "         [0.48627451, 0.48627451, 0.48627451],\n",
       "         [0.43921569, 0.43921569, 0.43921569],\n",
       "         [0.42745098, 0.42745098, 0.42745098]],\n",
       "\n",
       "        [[0.14901961, 0.14901961, 0.14901961],\n",
       "         [0.14509804, 0.14509804, 0.14509804],\n",
       "         [0.13333333, 0.13333333, 0.13333333],\n",
       "         ...,\n",
       "         [0.45098039, 0.45098039, 0.45098039],\n",
       "         [0.43529412, 0.43529412, 0.43529412],\n",
       "         [0.41568627, 0.41568627, 0.41568627]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.15686275, 0.15686275, 0.15686275],\n",
       "         [0.16078431, 0.16078431, 0.16078431],\n",
       "         [0.16862745, 0.16862745, 0.16862745],\n",
       "         ...,\n",
       "         [0.13333333, 0.13333333, 0.13333333],\n",
       "         [0.12941176, 0.12941176, 0.12941176],\n",
       "         [0.1372549 , 0.1372549 , 0.1372549 ]],\n",
       "\n",
       "        [[0.16078431, 0.16078431, 0.16078431],\n",
       "         [0.15294118, 0.15294118, 0.15294118],\n",
       "         [0.15294118, 0.15294118, 0.15294118],\n",
       "         ...,\n",
       "         [0.12156863, 0.12156863, 0.12156863],\n",
       "         [0.1254902 , 0.1254902 , 0.1254902 ],\n",
       "         [0.13333333, 0.13333333, 0.13333333]],\n",
       "\n",
       "        [[0.14117647, 0.14117647, 0.14117647],\n",
       "         [0.15686275, 0.15686275, 0.15686275],\n",
       "         [0.14901961, 0.14901961, 0.14901961],\n",
       "         ...,\n",
       "         [0.11372549, 0.11372549, 0.11372549],\n",
       "         [0.1254902 , 0.1254902 , 0.1254902 ],\n",
       "         [0.12156863, 0.12156863, 0.12156863]]],\n",
       "\n",
       "\n",
       "       [[[0.16862745, 0.16862745, 0.16862745],\n",
       "         [0.15294118, 0.15294118, 0.15294118],\n",
       "         [0.14509804, 0.14509804, 0.14509804],\n",
       "         ...,\n",
       "         [0.10980392, 0.10980392, 0.10980392],\n",
       "         [0.11764706, 0.11764706, 0.11764706],\n",
       "         [0.10980392, 0.10980392, 0.10980392]],\n",
       "\n",
       "        [[0.16470588, 0.16470588, 0.16470588],\n",
       "         [0.1372549 , 0.1372549 , 0.1372549 ],\n",
       "         [0.16078431, 0.16078431, 0.16078431],\n",
       "         ...,\n",
       "         [0.10980392, 0.10980392, 0.10980392],\n",
       "         [0.10980392, 0.10980392, 0.10980392],\n",
       "         [0.11764706, 0.11764706, 0.11764706]],\n",
       "\n",
       "        [[0.16470588, 0.16470588, 0.16470588],\n",
       "         [0.14509804, 0.14509804, 0.14509804],\n",
       "         [0.14117647, 0.14117647, 0.14117647],\n",
       "         ...,\n",
       "         [0.12156863, 0.12156863, 0.12156863],\n",
       "         [0.10196078, 0.10196078, 0.10196078],\n",
       "         [0.1254902 , 0.1254902 , 0.1254902 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.10980392, 0.10980392, 0.10980392],\n",
       "         [0.11764706, 0.11764706, 0.11764706],\n",
       "         [0.10196078, 0.10196078, 0.10196078],\n",
       "         ...,\n",
       "         [0.14117647, 0.14117647, 0.14117647],\n",
       "         [0.13333333, 0.13333333, 0.13333333],\n",
       "         [0.16078431, 0.16078431, 0.16078431]],\n",
       "\n",
       "        [[0.12156863, 0.12156863, 0.12156863],\n",
       "         [0.11372549, 0.11372549, 0.11372549],\n",
       "         [0.11372549, 0.11372549, 0.11372549],\n",
       "         ...,\n",
       "         [0.11764706, 0.11764706, 0.11764706],\n",
       "         [0.1372549 , 0.1372549 , 0.1372549 ],\n",
       "         [0.13333333, 0.13333333, 0.13333333]],\n",
       "\n",
       "        [[0.12156863, 0.12156863, 0.12156863],\n",
       "         [0.12156863, 0.12156863, 0.12156863],\n",
       "         [0.11372549, 0.11372549, 0.11372549],\n",
       "         ...,\n",
       "         [0.12156863, 0.12156863, 0.12156863],\n",
       "         [0.13333333, 0.13333333, 0.13333333],\n",
       "         [0.14509804, 0.14509804, 0.14509804]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.00784314, 0.00784314, 0.00784314],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.00784314, 0.00784314, 0.00784314],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.01568627, 0.01568627, 0.01568627],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.70588235, 0.70588235, 0.70588235],\n",
       "         [0.65490196, 0.65490196, 0.65490196],\n",
       "         [0.56078431, 0.56078431, 0.56078431],\n",
       "         ...,\n",
       "         [0.36862745, 0.36862745, 0.36862745],\n",
       "         [0.34901961, 0.34901961, 0.34901961],\n",
       "         [0.3254902 , 0.3254902 , 0.3254902 ]],\n",
       "\n",
       "        [[0.6       , 0.6       , 0.6       ],\n",
       "         [0.72941176, 0.72941176, 0.72941176],\n",
       "         [0.60784314, 0.60784314, 0.60784314],\n",
       "         ...,\n",
       "         [0.37254902, 0.37254902, 0.37254902],\n",
       "         [0.34901961, 0.34901961, 0.34901961],\n",
       "         [0.32156863, 0.32156863, 0.32156863]],\n",
       "\n",
       "        [[0.42352941, 0.42352941, 0.42352941],\n",
       "         [0.75686275, 0.75686275, 0.75686275],\n",
       "         [0.59607843, 0.59607843, 0.59607843],\n",
       "         ...,\n",
       "         [0.37647059, 0.37647059, 0.37647059],\n",
       "         [0.34509804, 0.34509804, 0.34509804],\n",
       "         [0.32156863, 0.32156863, 0.32156863]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_data), len(val_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, transform):\n",
    "        self.transform = transform\n",
    "        self.csv = csv\n",
    "        \n",
    "        self.image_name = self.csv[:]['image_name']\n",
    "        self.label = np.array(self.csv[:]['labels'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        images = Image.open(self.csv['path'].iloc[index] + self.image_name.iloc[index]).convert('RGB')\n",
    "        \n",
    "        images = self.transform(images)\n",
    "        targets = self.label[index]\n",
    "        targets = torch.tensor(targets, dtype = torch.long)\n",
    "#         sample = {'image': image, 'labels': targets}\n",
    "        \n",
    "        return images, targets\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(3),\n",
    "    transforms.Resize((224,224)),    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(train_data, transform)\n",
    "val_dataset = ImageDataset(val_data, transform)\n",
    "test_dataset = ImageDataset(test_data, transform)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.from_numpy(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trucloan/anaconda3/envs/me/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "label = torch.tensor(label, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 1, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform one-hot encoding all the labels\n",
    "label = F.one_hot(label, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.array([0.229, 0.224, 0.225])\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "data = datasets.ImageFolder(path_dataset, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_label, test_label=train_test_split(dataset, label, test_size=0.2, stratify = label,random_state = 42, shuffle=True)\n",
    "train_data, val_data, train_label, val_label=train_test_split(train_data, train_label, test_size=0.2, random_state = 42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400 1600 2000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(val_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0275, 0.0275, 0.0275],\n",
       "          [0.0275, 0.0275, 0.0275],\n",
       "          [0.0275, 0.0275, 0.0275],\n",
       "          ...,\n",
       "          [0.0314, 0.0314, 0.0314],\n",
       "          [0.0314, 0.0314, 0.0314],\n",
       "          [0.0314, 0.0314, 0.0314]],\n",
       "\n",
       "         [[0.0275, 0.0275, 0.0275],\n",
       "          [0.0275, 0.0275, 0.0275],\n",
       "          [0.0275, 0.0275, 0.0275],\n",
       "          ...,\n",
       "          [0.0314, 0.0314, 0.0314],\n",
       "          [0.0314, 0.0314, 0.0314],\n",
       "          [0.0275, 0.0275, 0.0275]],\n",
       "\n",
       "         [[0.0275, 0.0275, 0.0275],\n",
       "          [0.0275, 0.0275, 0.0275],\n",
       "          [0.0275, 0.0275, 0.0275],\n",
       "          ...,\n",
       "          [0.0275, 0.0275, 0.0275],\n",
       "          [0.0275, 0.0275, 0.0275],\n",
       "          [0.0275, 0.0275, 0.0275]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0471, 0.0471, 0.0471],\n",
       "          [0.0510, 0.0510, 0.0510],\n",
       "          [0.0471, 0.0471, 0.0471],\n",
       "          ...,\n",
       "          [0.9922, 0.9922, 0.9922],\n",
       "          [0.0392, 0.0392, 0.0392],\n",
       "          [0.0706, 0.0706, 0.0706]],\n",
       "\n",
       "         [[0.0275, 0.0275, 0.0275],\n",
       "          [0.0471, 0.0471, 0.0471],\n",
       "          [0.0510, 0.0510, 0.0510],\n",
       "          ...,\n",
       "          [0.0235, 0.0235, 0.0235],\n",
       "          [0.0275, 0.0275, 0.0275],\n",
       "          [0.0353, 0.0353, 0.0353]],\n",
       "\n",
       "         [[0.0510, 0.0510, 0.0510],\n",
       "          [0.0510, 0.0510, 0.0510],\n",
       "          [0.0510, 0.0510, 0.0510],\n",
       "          ...,\n",
       "          [0.0314, 0.0314, 0.0314],\n",
       "          [0.0275, 0.0275, 0.0275],\n",
       "          [0.0314, 0.0314, 0.0314]]],\n",
       "\n",
       "\n",
       "        [[[0.1255, 0.1255, 0.1255],\n",
       "          [0.0118, 0.0118, 0.0118],\n",
       "          [0.0039, 0.0039, 0.0039],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0118, 0.0118, 0.0118],\n",
       "          [0.1255, 0.1255, 0.1255]],\n",
       "\n",
       "         [[0.1255, 0.1255, 0.1255],\n",
       "          [0.0118, 0.0118, 0.0118],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0118, 0.0118, 0.0118],\n",
       "          [0.1255, 0.1255, 0.1255]],\n",
       "\n",
       "         [[0.1294, 0.1294, 0.1294],\n",
       "          [0.0157, 0.0157, 0.0157],\n",
       "          [0.0118, 0.0118, 0.0118],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0118, 0.0118, 0.0118],\n",
       "          [0.1255, 0.1255, 0.1255]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.2235, 0.2235, 0.2235],\n",
       "          [0.1373, 0.1373, 0.1373],\n",
       "          [0.1490, 0.1490, 0.1490],\n",
       "          ...,\n",
       "          [0.1294, 0.1294, 0.1294],\n",
       "          [0.1098, 0.1098, 0.1098],\n",
       "          [0.2157, 0.2157, 0.2157]],\n",
       "\n",
       "         [[0.1922, 0.1922, 0.1922],\n",
       "          [0.0941, 0.0941, 0.0941],\n",
       "          [0.1098, 0.1098, 0.1098],\n",
       "          ...,\n",
       "          [0.0941, 0.0941, 0.0941],\n",
       "          [0.0824, 0.0824, 0.0824],\n",
       "          [0.1882, 0.1882, 0.1882]],\n",
       "\n",
       "         [[0.3843, 0.3843, 0.3843],\n",
       "          [0.3020, 0.3020, 0.3020],\n",
       "          [0.3098, 0.3098, 0.3098],\n",
       "          ...,\n",
       "          [0.3098, 0.3098, 0.3098],\n",
       "          [0.3098, 0.3098, 0.3098],\n",
       "          [0.3843, 0.3843, 0.3843]]],\n",
       "\n",
       "\n",
       "        [[[0.9686, 0.9686, 0.9686],\n",
       "          [0.9412, 0.9412, 0.9412],\n",
       "          [0.8510, 0.8510, 0.8510],\n",
       "          ...,\n",
       "          [0.2902, 0.2902, 0.2902],\n",
       "          [0.2902, 0.2902, 0.2902],\n",
       "          [0.2588, 0.2588, 0.2588]],\n",
       "\n",
       "         [[0.8980, 0.8980, 0.8980],\n",
       "          [0.7608, 0.7608, 0.7608],\n",
       "          [0.5490, 0.5490, 0.5490],\n",
       "          ...,\n",
       "          [0.0784, 0.0784, 0.0784],\n",
       "          [0.0784, 0.0784, 0.0784],\n",
       "          [0.0627, 0.0627, 0.0627]],\n",
       "\n",
       "         [[0.7412, 0.7412, 0.7412],\n",
       "          [0.5020, 0.5020, 0.5020],\n",
       "          [0.2667, 0.2667, 0.2667],\n",
       "          ...,\n",
       "          [0.0196, 0.0196, 0.0196],\n",
       "          [0.0196, 0.0196, 0.0196],\n",
       "          [0.0157, 0.0157, 0.0157]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.9569, 0.9569, 0.9569],\n",
       "          [0.8667, 0.8667, 0.8667],\n",
       "          [0.7451, 0.7451, 0.7451],\n",
       "          ...,\n",
       "          [0.6000, 0.6000, 0.6000],\n",
       "          [0.4941, 0.4941, 0.4941],\n",
       "          [0.3922, 0.3922, 0.3922]],\n",
       "\n",
       "         [[0.9686, 0.9686, 0.9686],\n",
       "          [0.8863, 0.8863, 0.8863],\n",
       "          [0.7804, 0.7804, 0.7804],\n",
       "          ...,\n",
       "          [0.5765, 0.5765, 0.5765],\n",
       "          [0.5569, 0.5569, 0.5569],\n",
       "          [0.4667, 0.4667, 0.4667]],\n",
       "\n",
       "         [[0.9765, 0.9765, 0.9765],\n",
       "          [0.9137, 0.9137, 0.9137],\n",
       "          [0.8235, 0.8235, 0.8235],\n",
       "          ...,\n",
       "          [0.5882, 0.5882, 0.5882],\n",
       "          [0.5843, 0.5843, 0.5843],\n",
       "          [0.5451, 0.5451, 0.5451]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.0275, 0.0275, 0.0275],\n",
       "          [0.0353, 0.0353, 0.0353],\n",
       "          [0.0353, 0.0353, 0.0353],\n",
       "          ...,\n",
       "          [0.0196, 0.0196, 0.0196],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0275, 0.0275, 0.0275],\n",
       "          [0.0353, 0.0353, 0.0353],\n",
       "          [0.0353, 0.0353, 0.0353],\n",
       "          ...,\n",
       "          [0.0196, 0.0196, 0.0196],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0275, 0.0275, 0.0275],\n",
       "          [0.0353, 0.0353, 0.0353],\n",
       "          [0.0353, 0.0353, 0.0353],\n",
       "          ...,\n",
       "          [0.0196, 0.0196, 0.0196],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0039, 0.0039, 0.0039],\n",
       "          ...,\n",
       "          [0.0314, 0.0314, 0.0314],\n",
       "          [0.0314, 0.0314, 0.0314],\n",
       "          [0.0275, 0.0275, 0.0275]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0039, 0.0039, 0.0039],\n",
       "          ...,\n",
       "          [0.0078, 0.0078, 0.0078],\n",
       "          [0.0078, 0.0078, 0.0078],\n",
       "          [0.0078, 0.0078, 0.0078]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0039, 0.0039, 0.0039],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.4980, 0.4980, 0.4980],\n",
       "          [0.5333, 0.5333, 0.5333],\n",
       "          [0.5686, 0.5686, 0.5686],\n",
       "          ...,\n",
       "          [0.2667, 0.2667, 0.2667],\n",
       "          [0.2549, 0.2549, 0.2549],\n",
       "          [0.2431, 0.2431, 0.2431]],\n",
       "\n",
       "         [[0.5608, 0.5608, 0.5608],\n",
       "          [0.5255, 0.5255, 0.5255],\n",
       "          [0.5569, 0.5569, 0.5569],\n",
       "          ...,\n",
       "          [0.2941, 0.2941, 0.2941],\n",
       "          [0.2745, 0.2745, 0.2745],\n",
       "          [0.2667, 0.2667, 0.2667]],\n",
       "\n",
       "         [[0.5882, 0.5882, 0.5882],\n",
       "          [0.5137, 0.5137, 0.5137],\n",
       "          [0.5490, 0.5490, 0.5490],\n",
       "          ...,\n",
       "          [0.2824, 0.2824, 0.2824],\n",
       "          [0.2902, 0.2902, 0.2902],\n",
       "          [0.2863, 0.2863, 0.2863]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.7529, 0.7529, 0.7529],\n",
       "          [0.7647, 0.7647, 0.7647],\n",
       "          [0.7804, 0.7804, 0.7804],\n",
       "          ...,\n",
       "          [0.3725, 0.3725, 0.3725],\n",
       "          [0.3529, 0.3529, 0.3529],\n",
       "          [0.3333, 0.3333, 0.3333]],\n",
       "\n",
       "         [[0.7725, 0.7725, 0.7725],\n",
       "          [0.7843, 0.7843, 0.7843],\n",
       "          [0.7961, 0.7961, 0.7961],\n",
       "          ...,\n",
       "          [0.3843, 0.3843, 0.3843],\n",
       "          [0.3451, 0.3451, 0.3451],\n",
       "          [0.3294, 0.3294, 0.3294]],\n",
       "\n",
       "         [[0.7765, 0.7765, 0.7765],\n",
       "          [0.7961, 0.7961, 0.7961],\n",
       "          [0.7882, 0.7882, 0.7882],\n",
       "          ...,\n",
       "          [0.3569, 0.3569, 0.3569],\n",
       "          [0.3451, 0.3451, 0.3451],\n",
       "          [0.3294, 0.3294, 0.3294]]],\n",
       "\n",
       "\n",
       "        [[[0.4863, 0.4863, 0.4863],\n",
       "          [0.2392, 0.2392, 0.2392],\n",
       "          [0.0941, 0.0941, 0.0941],\n",
       "          ...,\n",
       "          [0.1647, 0.1647, 0.1647],\n",
       "          [0.2078, 0.2078, 0.2078],\n",
       "          [0.2275, 0.2275, 0.2275]],\n",
       "\n",
       "         [[0.1922, 0.1922, 0.1922],\n",
       "          [0.0549, 0.0549, 0.0549],\n",
       "          [0.0118, 0.0118, 0.0118],\n",
       "          ...,\n",
       "          [0.0157, 0.0157, 0.0157],\n",
       "          [0.0196, 0.0196, 0.0196],\n",
       "          [0.0275, 0.0275, 0.0275]],\n",
       "\n",
       "         [[0.0431, 0.0431, 0.0431],\n",
       "          [0.0039, 0.0039, 0.0039],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.5373, 0.5373, 0.5373],\n",
       "          [0.4706, 0.4706, 0.4706],\n",
       "          [0.4392, 0.4392, 0.4392],\n",
       "          ...,\n",
       "          [0.2706, 0.2706, 0.2706],\n",
       "          [0.2745, 0.2745, 0.2745],\n",
       "          [0.2627, 0.2627, 0.2627]],\n",
       "\n",
       "         [[0.5725, 0.5725, 0.5725],\n",
       "          [0.5098, 0.5098, 0.5098],\n",
       "          [0.4784, 0.4784, 0.4784],\n",
       "          ...,\n",
       "          [0.3176, 0.3176, 0.3176],\n",
       "          [0.3216, 0.3216, 0.3216],\n",
       "          [0.3020, 0.3020, 0.3020]],\n",
       "\n",
       "         [[0.6118, 0.6118, 0.6118],\n",
       "          [0.5529, 0.5529, 0.5529],\n",
       "          [0.5333, 0.5333, 0.5333],\n",
       "          ...,\n",
       "          [0.3647, 0.3647, 0.3647],\n",
       "          [0.3647, 0.3647, 0.3647],\n",
       "          [0.3529, 0.3529, 0.3529]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "    sampler= train_data, num_workers= 0)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "    sampler= val_data, num_workers= 0)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "    sampler= test_data, num_workers= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f7788a0a190>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the pretrained model\n",
    "model = models.vgg16(pretrained = True)\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove the pre-trained network's final layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.avgpool = nn.AdaptiveAvgPool2d(output_size=(4, 4))\n",
    "numFeatures = model.classifier[0].in_features\n",
    "# loop over the modules of the model and set the parameters of\n",
    "# batch normalization modules as not trainable\n",
    "for module, param in zip(model.modules(), model.parameters()):\n",
    "    #kiểm tra xem môt đối tượng (tham số thứ nhất) là một instance hay là một lớp con của tham số thứ hai.\n",
    "    if isinstance(module, nn.BatchNorm2d): \n",
    "        param.requires_grad = False\n",
    "# define the network head and attach it to the model\n",
    "headModel = nn.Sequential(\n",
    "    nn.Linear(numFeatures, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(64, num_classes)\n",
    ")\n",
    "model.classifier = headModel \n",
    "# append a new classification top to our feature extractor and pop it\n",
    "# on to the current device\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(4, 4))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "    (5): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define  Train the Model using Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, loss_list, acc_list):\n",
    "    model.to(device)\n",
    "    #List to store loss to visualize\n",
    "    lossli = loss_list\n",
    "    accli = acc_list\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    valid_loss_min = np.Inf # track change in validation loss\n",
    "    count = 0\n",
    "    patience = 8 # nếu val_loss tăng 15 lần thì ngừng\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        train_acc = 0.0\n",
    "        valid_acc = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        \n",
    "        model.train()\n",
    "        for data, label in tqdm(train_loader):\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            \n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            \n",
    "            _, pred = torch.max(output, 1)              \n",
    "            \n",
    "            train_acc += pred.eq(target).sum().item()\n",
    "            \n",
    "#         scheduler.step() ###########\n",
    "            \n",
    "        ######################\n",
    "        # validate the model #\n",
    "        ######################\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, label in tqdm(val_loader):\n",
    "                data = data.to(device)\n",
    "                label = label.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, pred = torch.max(output, 1)\n",
    "#                 y_true += target.tolist()\n",
    "#                 y_pred += pred.tolist()  \n",
    "                \n",
    "                valid_acc +=  pred.eq(target).sum().item()\n",
    "       \n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(val_loader.dataset)\n",
    "        lossli.append({'epoch':epoch,'train_loss': train_loss,'valid_loss':valid_loss})\n",
    "        \n",
    "        train_acc = train_acc*100/len(train_loader.dataset)\n",
    "        valid_acc = valid_acc*100/len(val_loader.dataset)\n",
    "        accli.append({'epoch':epoch,'train_acc': train_acc,'valid_acc':valid_acc})\n",
    "        \n",
    "        ####################\n",
    "        # Early stopping #\n",
    "        ##################\n",
    "        \n",
    "        # print training/validation statistics\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))\n",
    "        # save model if validation loss has decreased\n",
    "       \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_acc': accli,\n",
    "            'loss_list': lossli,\n",
    "            'loss': loss\n",
    "            }, CHECKPOINT_PATH)\n",
    "        \n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model ...'.format(\n",
    "                valid_loss_min,\n",
    "                valid_loss))\n",
    "            \n",
    "            count = 0\n",
    "            print('count = ',count)\n",
    "            torch.save(model, './VGG16_model.pt') #save model \n",
    "                                  \n",
    "            valid_loss_min = valid_loss\n",
    "        else:\n",
    "            count += 1\n",
    "            print('count = ',count)\n",
    "            if count >= patience:\n",
    "                print('Early stopping!')\n",
    "   \n",
    "                return lossli, accli    \n",
    "           \n",
    "    return lossli, accli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t features.0.weight\n",
      "\t features.0.bias\n",
      "\t features.2.weight\n",
      "\t features.2.bias\n",
      "\t features.5.weight\n",
      "\t features.5.bias\n",
      "\t features.7.weight\n",
      "\t features.7.bias\n",
      "\t features.10.weight\n",
      "\t features.10.bias\n",
      "\t features.12.weight\n",
      "\t features.12.bias\n",
      "\t features.14.weight\n",
      "\t features.14.bias\n",
      "\t features.17.weight\n",
      "\t features.17.bias\n",
      "\t features.19.weight\n",
      "\t features.19.bias\n",
      "\t features.21.weight\n",
      "\t features.21.bias\n",
      "\t features.24.weight\n",
      "\t features.24.bias\n",
      "\t features.26.weight\n",
      "\t features.26.bias\n",
      "\t features.28.weight\n",
      "\t features.28.bias\n",
      "\t classifier.0.weight\n",
      "\t classifier.0.bias\n",
      "\t classifier.2.weight\n",
      "\t classifier.2.bias\n",
      "\t classifier.5.weight\n",
      "\t classifier.5.bias\n"
     ]
    }
   ],
   "source": [
    "#Create the optimizer\n",
    "params_to_update = model.parameters()\n",
    "print('Params to learn:')\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print('\\t',name)\n",
    "else:\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print('\\t',name)\n",
    "            \n",
    "optimizer = Adam(params_to_update ,lr = 0.001, weight_decay=1e-5)\n",
    "# scheduler = lr_scheduler.MultiStepLR(optimizer_ft, milestones = [20, 35], gamma=0.1, last_epoch=-1, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-f93e943560a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0macc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0mtime_elapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msince\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-059cde48b1bd>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, optimizer, loss_list, acc_list)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/me/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1105\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/me/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/me/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/me/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/me/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tensors used as indices must be long, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "loss_list, acc_list = [],[]\n",
    "\n",
    "since = time.time()\n",
    "\n",
    "loss, acc = training_loop(\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    loss_list = loss_list,\n",
    "    acc_list = acc_list\n",
    ")\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model, state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(CHECKPOINT_PATH)#, map_location=device)\n",
    "model_ft.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer_ft.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss_list = checkpoint['loss_list']\n",
    "acc_list = checkpoint['train_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ve do thi loss (train, val)\n",
    "def visualize_loss (checkpoint, path_loss):\n",
    "    loss = checkpoint['loss_list'] \n",
    "    train_loss = [x['train_loss'] for x in loss]\n",
    "    valid_loss = [x['valid_loss'] for x in loss]\n",
    "    fig, ax = plt.subplots(figsize = (18, 14.5))\n",
    "    ax.plot(train_loss, '-gx', label='Training loss')\n",
    "    ax.plot(valid_loss , '-ro', label='Validation loss')\n",
    "    ax.set(title=\"Loss over epochs of Model \",\n",
    "    xlabel='Epoch',\n",
    "    ylabel='Loss')\n",
    "    ax.legend()\n",
    "    fig.show()\n",
    "    plt.savefig(path_loss)\n",
    "visualize_loss(checkpoint, './loss2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ve do thi acc (train, val)\n",
    "def visualize_acc (checkpoint, path_acc):\n",
    "    acclist = checkpoint['train_acc'] #\n",
    "    train_acc = [x['train_acc'] for x in acclist]\n",
    "    valid_acc = [x['valid_acc'] for x in acclist]\n",
    "    fig, ax = plt.subplots(figsize = (18, 14.5))\n",
    "    ax.plot(train_acc, '-bx', label='Training acc')\n",
    "    ax.plot(valid_acc , '-yo', label='Validation acc')\n",
    "    ax.set(title=\"Accuracy over epochs of Model \",\n",
    "    xlabel='Epoch',\n",
    "    ylabel='Accuracy')\n",
    "    ax.legend()\n",
    "    fig.show()\n",
    "    plt.savefig(path_acc)\n",
    "visualize_acc(checkpoint, './Accuracy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test model\n",
    "def test_loop():\n",
    "    with torch.no_grad():\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        model_ft.to(device)\n",
    "        model_ft.eval()\n",
    "        for sample in test_dataloader:\n",
    "            batch_size = sample['image'].size(0)\n",
    "            data = sample['image'].to(device)\n",
    "            target = sample['labels'].to(device)\n",
    "            output = model_ft(data)\n",
    "            _,pred = torch.max(output, 1)\n",
    "            y_true += target.tolist()\n",
    "            y_pred += pred.tolist()\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "y_true, y_pred = test_loop()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "fix, ax = plt.subplots(figsize = (10,10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cnf_matrix, display_labels = classes)\n",
    "disp.plot(include_values = True, cmap = 'viridis_r', ax = ax, xticks_rotation = 'vertical')\n",
    "plt.savefig('MatrixFT.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_rp = './report_FTVGG19bn.txt'\n",
    "try:\n",
    "    s = classification_report(y_true, y_pred, target_names = classes)\n",
    "    with open(path_rp, mode ='x') as f:\n",
    "        f.write(s)\n",
    "    with open(path_rp) as f:\n",
    "        print(f.read())\n",
    "    f.close()\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict images\n",
    "def img_transform(path_img):\n",
    "    img = Image.open(path_img)\n",
    "    imagetensor = test_transform(img).cuda()\n",
    "    return imagetensor\n",
    "\n",
    "def predict(path_img, verbose = False):\n",
    "    if not verbose:\n",
    "        warnings.filterwarnings('ignore')\n",
    "    try:\n",
    "        checks_if_model_is_loaded = type(model)\n",
    "    except:\n",
    "        pass\n",
    "    model.eval()\n",
    "    if verbose:\n",
    "        print('Model loader ...')\n",
    "    image = img_transform(path_img)\n",
    "    image1 = image[None,:,:,:]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_ft(image1)\n",
    "        \n",
    "        _,pred_int = torch.max(outputs.data, 1)\n",
    "        _,top1_idx = torch.topk(outputs.data, 1, dim = 1)\n",
    "        pred_idx = int(pred_int.cpu().numpy())\n",
    "        if pred_idx == 0:\n",
    "            pred_str = str('Negative')\n",
    "            print('img: {} is: {}'.format(os.path.basename(path_img), pred_str))\n",
    "        else:\n",
    "            pred_str = str('Positive')\n",
    "            print('img: {} is: {}'.format(os.path.basename(path_img), pred_str))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_str = str('')\n",
    "path_image = './pred/covid.jpg'\n",
    "\n",
    "img = Image.open(path_image)\n",
    "plt.imshow(img)\n",
    "\n",
    "predict(path_image)\n",
    "plt.title('predict:{}'.format(pred_str))\n",
    "plt.text(5,45,'top {}:{}'.format(1,pred_str), bbox = dict(fc='yellow'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
